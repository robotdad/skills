#!/usr/bin/env python3
"""
Download images from Wikipedia or Wikimedia Commons using MediaWiki API.

This production-ready script handles Wikipedia's MD5-based directory hashing
by using the MediaWiki API to get actual image URLs. Never construct Wikipedia
image URLs manually - they will not work!

Generated by: image-search skill
Purpose: Batch download Wikipedia images with progress reporting

Usage:
    # Download specific files
    python download_wikipedia_images.py --filenames File1.jpg File2.jpg
    
    # Download from a list file
    python download_wikipedia_images.py --from-list filenames.txt
    
    # Download from Commons instead of Wikipedia
    python download_wikipedia_images.py --site commons.wikimedia.org --filenames Image.jpg
    
    # Specify output directory
    python download_wikipedia_images.py --output-dir ./posters --filenames Poster.jpg
"""

import urllib.request
import urllib.parse
import json
import argparse
import sys
import os
import time
from pathlib import Path


def get_wikipedia_image_url(filename, site='en.wikipedia.org'):
    """
    Get actual Wikipedia/Commons image URL using MediaWiki API.
    
    Wikipedia uses MD5-based directory hashing which makes manual URL
    construction impossible. Always use this API method.
    
    Args:
        filename: Image filename (with or without 'File:' prefix)
                 Examples: "Dawn_of_the_dead.jpg" or "File:Dawn_of_the_dead.jpg"
        site: 'en.wikipedia.org' for Wikipedia, 'commons.wikimedia.org' for Commons
    
    Returns:
        Direct download URL or None if not found
        
    Example URL returned:
        https://upload.wikimedia.org/wikipedia/en/6/63/Dawn_of_the_dead.jpg
    """
    # Remove 'File:' prefix if present
    if filename.startswith('File:'):
        filename = filename[5:]
    
    api_url = f'https://{site}/w/api.php?' + urllib.parse.urlencode({
        'action': 'query',
        'titles': f'File:{filename}',
        'prop': 'imageinfo',
        'iiprop': 'url|size',  # Get URL and size info
        'format': 'json'
    })
    
    headers = {'User-Agent': 'Research/1.0'}
    req = urllib.request.Request(api_url, headers=headers)
    
    try:
        response = urllib.request.urlopen(req, timeout=10)
        data = json.loads(response.read())
        pages = data['query']['pages']
        page_id = list(pages.keys())[0]
        
        if page_id != '-1' and 'imageinfo' in pages[page_id]:
            image_info = pages[page_id]['imageinfo'][0]
            return {
                'url': image_info['url'],
                'size': image_info.get('size', 0)
            }
    except Exception as e:
        print(f"API Error for {filename}: {e}", file=sys.stderr)
    
    return None


def download_image(url, output_path, show_progress=True):
    """
    Download image with proper headers and error handling.
    
    Args:
        url: Direct image URL
        output_path: Local file path to save image
        show_progress: Whether to show progress messages
    
    Returns:
        Tuple of (success: bool, size_kb: float, error_msg: str)
    """
    headers = {'User-Agent': 'Research/1.0'}
    req = urllib.request.Request(url, headers=headers)
    
    try:
        with urllib.request.urlopen(req, timeout=30) as response:
            data = response.read()
            
            # Write to file
            with open(output_path, 'wb') as f:
                f.write(data)
            
            size_kb = len(data) / 1024
            
            if show_progress:
                print(f"✓ {output_path}: {size_kb:.1f} KB")
                sys.stdout.flush()
            
            return True, size_kb, None
            
    except Exception as e:
        error_msg = str(e)
        if show_progress:
            print(f"✗ {output_path}: {error_msg}", file=sys.stderr)
            sys.stdout.flush()
        return False, 0, error_msg


def sanitize_filename(filename):
    """
    Sanitize filename for safe filesystem usage.
    
    Args:
        filename: Original filename
    
    Returns:
        Safe filename for local filesystem
    """
    # Remove 'File:' prefix if present
    if filename.startswith('File:'):
        filename = filename[5:]
    
    # Replace problematic characters
    safe = filename.replace(' ', '_')
    
    # Keep only alphanumeric, underscore, hyphen, dot
    safe = ''.join(c for c in safe if c.isalnum() or c in '._-')
    
    return safe


def load_filenames_from_file(filepath):
    """
    Load filenames from a text file (one per line).
    
    Args:
        filepath: Path to text file containing filenames
    
    Returns:
        List of filenames (empty lines and comments skipped)
    """
    filenames = []
    
    try:
        with open(filepath, 'r') as f:
            for line in f:
                line = line.strip()
                # Skip empty lines and comments
                if line and not line.startswith('#'):
                    filenames.append(line)
    except Exception as e:
        print(f"Error reading file {filepath}: {e}", file=sys.stderr)
        sys.exit(1)
    
    return filenames


def main():
    parser = argparse.ArgumentParser(
        description='Download images from Wikipedia or Wikimedia Commons',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Download specific files from Wikipedia
  python download_wikipedia_images.py --filenames Dawn_of_the_dead.jpg Night_of_the_Living_Dead_poster.jpg
  
  # Download from a list file
  python download_wikipedia_images.py --from-list my_images.txt
  
  # Download from Wikimedia Commons
  python download_wikipedia_images.py --site commons.wikimedia.org --filenames Golden_Gate_Bridge.jpg
  
  # Specify output directory
  python download_wikipedia_images.py --output-dir ./downloads --filenames Image.jpg

Note: Wikipedia image filenames can be found on article pages (look for File:*.jpg references)
        """
    )
    
    # Input sources (mutually exclusive)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        '--filenames',
        nargs='+',
        help='Image filenames to download (space-separated)'
    )
    input_group.add_argument(
        '--from-list',
        metavar='FILE',
        help='Text file with filenames (one per line)'
    )
    
    # Options
    parser.add_argument(
        '--site',
        default='en.wikipedia.org',
        choices=['en.wikipedia.org', 'commons.wikimedia.org'],
        help='Site to download from (default: en.wikipedia.org)'
    )
    parser.add_argument(
        '--output-dir',
        default='.',
        help='Output directory for downloaded images (default: current directory)'
    )
    parser.add_argument(
        '--delay',
        type=float,
        default=1.0,
        help='Delay between downloads in seconds (default: 1.0)'
    )
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='Suppress progress output'
    )
    
    args = parser.parse_args()
    
    # Get list of filenames
    if args.filenames:
        filenames = args.filenames
    else:
        filenames = load_filenames_from_file(args.from_list)
    
    if not filenames:
        print("Error: No filenames to download", file=sys.stderr)
        sys.exit(1)
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Statistics
    total = len(filenames)
    successful = 0
    failed = 0
    total_size_kb = 0
    
    # Print header
    if not args.quiet:
        print(f"Downloading {total} image(s) from {args.site}...")
        print(f"Output directory: {output_dir}")
        print()
        sys.stdout.flush()
    
    # Download each file
    for idx, filename in enumerate(filenames, 1):
        if not args.quiet:
            print(f"[{idx}/{total}] Processing: {filename}")
            sys.stdout.flush()
        
        # Get actual URL via MediaWiki API
        result = get_wikipedia_image_url(filename, args.site)
        
        if not result:
            print(f"✗ Could not find: {filename}", file=sys.stderr)
            sys.stdout.flush()
            failed += 1
            continue
        
        # Sanitize filename for local filesystem
        safe_filename = sanitize_filename(filename)
        output_path = output_dir / safe_filename
        
        # Download the image
        success, size_kb, error = download_image(
            result['url'],
            str(output_path),
            show_progress=not args.quiet
        )
        
        if success:
            successful += 1
            total_size_kb += size_kb
        else:
            failed += 1
        
        # Be respectful to Wikipedia's servers
        if idx < total and args.delay > 0:
            time.sleep(args.delay)
    
    # Print summary
    if not args.quiet:
        print()
        print("=" * 60)
        print("Download Summary:")
        print(f"  Total: {total}")
        print(f"  Successful: {successful}")
        print(f"  Failed: {failed}")
        print(f"  Total size: {total_size_kb:.1f} KB ({total_size_kb/1024:.2f} MB)")
        print(f"  Output directory: {output_dir}")
        print("=" * 60)
    
    # Exit with error code if any failed
    sys.exit(0 if failed == 0 else 1)


if __name__ == "__main__":
    main()
